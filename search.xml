<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>吴恩达机器学习笔记2</title>
      <link href="/2021/03/09/Machine%20Learning2/"/>
      <url>/2021/03/09/Machine%20Learning2/</url>
      
        <content type="html"><![CDATA[<h2 id="Chapter-2-Liner-regression-with-one-variable"><a href="#Chapter-2-Liner-regression-with-one-variable" class="headerlink" title="Chapter 2 - Liner regression with one variable"></a>Chapter 2 - Liner regression with one variable</h2><p>[TOC]</p><h3 id="2-1-Model-Representation"><a href="#2-1-Model-Representation" class="headerlink" title="2.1 Model Representation"></a>2.1 Model Representation</h3><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/imgimage-20210303005018800.png" alt="image-20210303005018800" style="zoom:80%;" /><p>图中x轴表示房屋尺寸，y轴表示房屋价格</p><p>假设你现在想出售一间房，面积是1250 feet^2，那么你可能卖到多少钱？</p><p>这是一个典型的监督学习问题，因为按照如下定义，我们对于样本有明确的答案。</p><blockquote><p><strong>Supervised Learning:</strong> Given the “right answer” for each example in the data</p></blockquote><p>同时，这也是一个回归问题，我们希望要去预测真实值。</p><blockquote><p><strong>Regression Problem</strong>: Predict real-valued output</p></blockquote><p>BTW： the other most common type of supervised learning problem is called the classification problem.</p><p><strong>Notation:</strong></p><ol><li>m – Number of training examples</li><li>x’s – “input” variable / features</li><li>y’s – “output” variable / “target” variable</li><li>(x,y) – one training example</li><li>(x^(i), y^(i)) – ith training example</li></ol><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/imgimage-20210303010410488.png" alt="image-20210303010410488"></p><ul><li><p>有训练集之后，我们可以通过算法，构建hypothesis 公式 h，通过h我们预测不同尺寸的房子的价格。</p></li><li><p>函数h通过x映射到y</p></li><li><p>h可以表示为 hθ(x) = θ0+θ1*x</p></li></ul><h3 id="2-2-Cost-Function"><a href="#2-2-Cost-Function" class="headerlink" title="2.2 Cost Function"></a>2.2 Cost Function</h3><p>How to choose θ0 and  θ1？</p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304002428330.png" alt="image-20210304002428330" style="zoom:75%;" /><p>在线性回归模型中，我们有一个训练集</p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304002509087.png" alt="image-20210304002509087"></p><p>我们现在想要得出 θ0 和  θ1的值，来让假设函数表示的直线尽量与这些数据点更好的拟合</p><p>思路：能使h(x)，也就是输入x时，我们预测的值<strong>最接近</strong>该样本对应的y值的参数 <strong>θ0 和 θ1</strong>。</p><p>即 minimize   1/2m * 求和[ ( hθ(x)^(i) - y^(i) ) ^2 ] (从i=1开始到m结束)</p><p><u>（弹幕提醒：1/2是为了消除2次方求导后的系数2）</u></p><p>预测值（预测价格）和实际值（实际卖出价格）的差的平方误差最小</p><p>改写上述函数为代价函数lost function</p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304003739145.png" alt="image-20210304003739145"></p><p>代价函数也被称为均方误差函数（squared error function），有时也被称为均方误差代价函数（squared error cost function），对于大多数问题，特别是回归问题都是一个合理的选择。</p><h3 id="2-3-Cost-Function-Intuition-I"><a href="#2-3-Cost-Function-Intuition-I" class="headerlink" title="2.3 Cost Function - Intuition I"></a>2.3 Cost Function - Intuition I</h3><p>What is the cost function and why do we want to use it ?</p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304004338106.png" alt=""></p><p>我们简化假设函数，失去常数项（<u>弹幕：求极限要求导，常数项并不影响</u>）</p><p>假设函数是为了修正theta1，关于X的函数；代价函数是关于theta1的函数，控制斜线的斜率。</p><p>我们先假设theta1=1，那么计算代价函数可以发现=0，即 J(1) = 0；</p><p>假设theta1=0.5，计算出 J(0.5) = 0.58;</p><p>假设theta1= 0，计算出 J(0) = 2.3</p><p>然后就可以在坐标轴中画出代价函数的图</p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304005011646.png" alt="image-20210304005011646"></p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304005339038.png" alt="image-20210304005339038"></p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304005551621.png" alt="image-20210304005551621"></p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304005646571.png" alt="image-20210304005646571"></p><p>经过一系列计算，你会得到这些点</p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304005712172.png" alt="image-20210304005712172">)<img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304005749552.png" alt="image-20210304005749552"></p><p>学习算法的优化目标是我们通过选择theta1的值，来获得最小的 J(theta1)</p><h3 id="2-4-Cost-Function-Intuition-II"><a href="#2-4-Cost-Function-Intuition-II" class="headerlink" title="2.4 Cost Function - Intuition II"></a>2.4 Cost Function - Intuition II</h3><p>上一节，我们舍去theta0，得到一个碗形的曲线。事实上，如果我们同时考虑theta0和theta1，我们还是会得到一个碗形的3D曲面图。轴标为theta0和theta1，当你改变这两个参数，你会得到不同的  J(theta0, theta1)的值，即曲面的高度。</p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304010642747.png" alt="image-20210304010642747" style="zoom: 60%;" /><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304010338028.png" alt="image-20210304010338028" style="zoom: 50%;" /></p><p>用等高线图（contour plots / figures）来表示这个3D图形</p><p>同一个椭圆上的点有相同的 J(theta0, theta1) 的值，如图粉红色的3个点</p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304011002810.png" alt="image-20210304011002810"></p><p>观察如下这个点，theta0约为800，theta1约为-0.1，对应蓝色的斜线，当然它并不能很好地拟合这些数据。等高线图中，这个点与最小值（中心点）的距离很远，这是一个很高的代价（cost），因为拟合得并不好</p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304011238343.png" alt="image-20210304011238343"></p><p>另一个例子，这个点theta0大约为360，theta1为0，是蓝色线表示的一条平行线。当然它的代价也很大，比上个点要小一些。</p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304011646560.png" alt="image-20210304011646560"></p><p><strong>Summary</strong></p><ul><li><p>我们真正想要的是一个高效的算法，来自动寻找代价函数 J 的最小值，对应的theta0 and theta1。</p></li><li><p>有更多的参数后，我们涉及更高维的图形，没有办法绘制图形，因为很难可视化。</p></li><li><p>我们需要利用软件去找到使函数最小的theta0和theta1。</p></li></ul><h3 id="2-5-Gradient-Descent"><a href="#2-5-Gradient-Descent" class="headerlink" title="2.5 Gradient Descent"></a>2.5 Gradient Descent</h3><p>思路：给定θ0和θ1的初始值，通常将设为0，在梯度下降算法中，不停地一点点改变θ0和θ1，来使 J(θ0,θ1)最小。</p><p>对θ0和θ1赋值，就相当于从这个函数表面上的某个点出发，把这个图像想象成一座山，你正站在一个点上，在梯度下降算法中，你旋转360°，看看周围，并问自己，如果我要在某个方向上走一小步，我想尽快下山，那么我应该朝什么方向迈步。</p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210305002044805.png" alt="image-20210305002044805" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210305002140272.png" alt="image-20210305002140272" style="zoom:50%;" /></p><p>但是如果起点不同，你可能会得到一个完全不同的局部最优解，这是梯度下降算法的一个特点。</p><p>梯度下降算法的定义：</p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210305002314334.png" alt="image-20210305002314334"></p><p>我们将会反复做这一步，直到收敛。</p><p>我们要更新参数θ_j，为θ_j 减去 α乘以这一部分</p><p>Notations:</p><ul><li>:= 表示赋值</li><li>a=b 表示判断ab是否相等</li><li>α被称为学习率(learning rate)，α用来控制梯度下降时，我们迈出多大的步子，如果α很大，梯度下降就很迅速</li><li>最后一部分是导数项</li></ul><p>梯度下降需要同时更新(Spontaneous update)θ0和θ1。右边的是错误的梯度下降算法。</p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210305004428576.png" alt="image-20210305004428576"></p><h3 id="2-6-Gradient-Descent-Intuition"><a href="#2-6-Gradient-Descent-Intuition" class="headerlink" title="2.6 Gradient Descent Intuition"></a>2.6 Gradient Descent Intuition</h3><p><strong>如何理解导数项？</strong></p><p>我们先假设只有一个参数 θ_1，并且θ_1是实数，那么  J(θ_1)的函数如下图所示：</p><ul><li>如果从上图的红点出发，导数项为那一个点的切线的斜率，而这个斜率是正数，α作为学习率也是正数，那么 θ_1赋值后会变小，即向左移动，更靠近最小值的点；</li><li>同理，如果从下图出发，导数项为负值，那么θ_1赋值后会变大，向右移动，也是更靠近最小值的点。</li></ul><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210305005542464.png" alt="image-20210305005542464"></p><p><strong>如何理解α？</strong></p><ul><li><p>如果α太小，需要很多步才能到达最低点（注意：越往后斜率越小）</p></li><li><p>如果α太大，那么梯度下降可能会越过最低点，甚至无法收敛或者发散</p></li></ul><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210305010049784.png" alt="image-20210305010049784"></p><p><strong>如果已经在局部最低点了呢？</strong></p><ul><li>那么斜率为0，导数项则为0，θ_1更新为本身，不会再改变</li></ul><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210305010222992.png" alt="image-20210305010222992"></p><p><strong>即使α不变，梯度下降法也可以收敛到局部最低点</strong></p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210308234402470.png" alt="image-20210308234402470"></p><p>随着越来越接近最低点，导数越来越小，梯度下降法移动的幅度会自动变得越来越小，知道最终移动幅度变的很小，你会发现，已经收敛到局部极小值了。</p><h3 id="2-7-Gradient-Descent-For-Linear-Regression"><a href="#2-7-Gradient-Descent-For-Linear-Regression" class="headerlink" title="2.7 Gradient Descent For Linear Regression"></a>2.7 Gradient Descent For Linear Regression</h3><p>梯度下降算法和线性回归模型</p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210308234650379.png" alt="image-20210308234650379"></p><p><strong>如何将梯度下降算法代入到平方差代价函数中，使其最小化呢？</strong></p><ul><li>重要的是偏导数项是什么？</li><li>下图进行推导</li></ul><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210309003956855.png" alt="image-20210309003956855"></p><p><strong>弹幕指路：</strong><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210309004751046.png" alt="image-20210309004751046"></p><ul><li>带回梯度下降算法，不断重复该过程直到收敛</li></ul><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210309004114921.png" alt="image-20210309004114921" style="zoom:80%;" /><ul><li>注意：当你用梯度下降时，确保θ_0和θ_1同时更新</li></ul><p>我们使用梯度下降法时，最容易遇到的问题是它容易陷入局部最优解</p><p>但是事实上，线性回归的代价函数总是这样一个碗状函数（凸函数 convex function），这个函数只有一个全局最优。当你计算这种代价函数的梯度下降时，它总是会收敛到全局最优。</p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210309004506784.png" alt="image-20210309004506784" style="zoom:50%;" /><p>刚才的算法，有时被称为<strong>Batch梯度下降算法</strong>，意味着每一步梯度下降，我们都遍历了训练集的所有样本，当计算偏导数时，我们计算总和。在每一个单独的梯度下降，我们最终计算m个训练样本的总和。</p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210309005136634.png" alt="image-20210309005136634" style="zoom:80%;" /><p>还有一种解法，求解代价函数 J 最小值，而不需要使用梯度下降这样的迭代算法，被称为正规方程租方法。</p><p>事实上相比于正规方程组法，梯度下降适用于更大的数据集。</p><p> <strong>❀❀❀ 恭喜你学习了第一个机器学习算法！ ❀❀❀</strong></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning - Andrew Ng </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Learning Notes </tag>
            
            <tag> Andrew Ng </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>吴恩达机器学习笔记2</title>
      <link href="/2021/03/09/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02/"/>
      <url>/2021/03/09/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02/</url>
      
        <content type="html"><![CDATA[<h2 id="Chapter-2-Liner-regression-with-one-variable"><a href="#Chapter-2-Liner-regression-with-one-variable" class="headerlink" title="Chapter 2 - Liner regression with one variable"></a>Chapter 2 - Liner regression with one variable</h2><p>[TOC]</p><h3 id="2-1-Model-Representation"><a href="#2-1-Model-Representation" class="headerlink" title="2.1 Model Representation"></a>2.1 Model Representation</h3><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/imgimage-20210303005018800.png" alt="image-20210303005018800" style="zoom:80%;" /><p>图中x轴表示房屋尺寸，y轴表示房屋价格</p><p>假设你现在想出售一间房，面积是1250 feet^2，那么你可能卖到多少钱？</p><p>这是一个典型的监督学习问题，因为按照如下定义，我们对于样本有明确的答案。</p><blockquote><p><strong>Supervised Learning:</strong> Given the “right answer” for each example in the data</p></blockquote><p>同时，这也是一个回归问题，我们希望要去预测真实值。</p><blockquote><p><strong>Regression Problem</strong>: Predict real-valued output</p></blockquote><p>BTW： the other most common type of supervised learning problem is called the classification problem.</p><p><strong>Notation:</strong></p><ol><li>m – Number of training examples</li><li>x’s – “input” variable / features</li><li>y’s – “output” variable / “target” variable</li><li>(x,y) – one training example</li><li>(x^(i), y^(i)) – ith training example</li></ol><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/imgimage-20210303010410488.png" alt="image-20210303010410488"></p><ul><li><p>有训练集之后，我们可以通过算法，构建hypothesis 公式 h，通过h我们预测不同尺寸的房子的价格。</p></li><li><p>函数h通过x映射到y</p></li><li><p>h可以表示为 hθ(x) = θ0+θ1*x</p></li></ul><h3 id="2-2-Cost-Function"><a href="#2-2-Cost-Function" class="headerlink" title="2.2 Cost Function"></a>2.2 Cost Function</h3><p>How to choose θ0 and  θ1？</p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304002428330.png" alt="image-20210304002428330" style="zoom:75%;" /><p>在线性回归模型中，我们有一个训练集</p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304002509087.png" alt="image-20210304002509087"></p><p>我们现在想要得出 θ0 和  θ1的值，来让假设函数表示的直线尽量与这些数据点更好的拟合</p><p>思路：能使h(x)，也就是输入x时，我们预测的值<strong>最接近</strong>该样本对应的y值的参数 <strong>θ0 和 θ1</strong>。</p><p>即 minimize   1/2m * 求和[ ( hθ(x)^(i) - y^(i) ) ^2 ] (从i=1开始到m结束)</p><p><u>（弹幕提醒：1/2是为了消除2次方求导后的系数2）</u></p><p>预测值（预测价格）和实际值（实际卖出价格）的差的平方误差最小</p><p>改写上述函数为代价函数lost function</p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304003739145.png" alt="image-20210304003739145"></p><p>代价函数也被称为均方误差函数（squared error function），有时也被称为均方误差代价函数（squared error cost function），对于大多数问题，特别是回归问题都是一个合理的选择。</p><h3 id="2-3-Cost-Function-Intuition-I"><a href="#2-3-Cost-Function-Intuition-I" class="headerlink" title="2.3 Cost Function - Intuition I"></a>2.3 Cost Function - Intuition I</h3><p>What is the cost function and why do we want to use it ?</p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304004338106.png" alt=""></p><p>我们简化假设函数，失去常数项（<u>弹幕：求极限要求导，常数项并不影响</u>）</p><p>假设函数是为了修正theta1，关于X的函数；代价函数是关于theta1的函数，控制斜线的斜率。</p><p>我们先假设theta1=1，那么计算代价函数可以发现=0，即 J(1) = 0；</p><p>假设theta1=0.5，计算出 J(0.5) = 0.58;</p><p>假设theta1= 0，计算出 J(0) = 2.3</p><p>然后就可以在坐标轴中画出代价函数的图</p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304005011646.png" alt="image-20210304005011646"></p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304005339038.png" alt="image-20210304005339038"></p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304005551621.png" alt="image-20210304005551621"></p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304005646571.png" alt="image-20210304005646571"></p><p>经过一系列计算，你会得到这些点</p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304005712172.png" alt="image-20210304005712172">)<img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304005749552.png" alt="image-20210304005749552"></p><p>学习算法的优化目标是我们通过选择theta1的值，来获得最小的 J(theta1)</p><h3 id="2-4-Cost-Function-Intuition-II"><a href="#2-4-Cost-Function-Intuition-II" class="headerlink" title="2.4 Cost Function - Intuition II"></a>2.4 Cost Function - Intuition II</h3><p>上一节，我们舍去theta0，得到一个碗形的曲线。事实上，如果我们同时考虑theta0和theta1，我们还是会得到一个碗形的3D曲面图。轴标为theta0和theta1，当你改变这两个参数，你会得到不同的  J(theta0, theta1)的值，即曲面的高度。</p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304010642747.png" alt="image-20210304010642747" style="zoom: 60%;" /><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304010338028.png" alt="image-20210304010338028" style="zoom: 50%;" /></p><p>用等高线图（contour plots / figures）来表示这个3D图形</p><p>同一个椭圆上的点有相同的 J(theta0, theta1) 的值，如图粉红色的3个点</p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304011002810.png" alt="image-20210304011002810"></p><p>观察如下这个点，theta0约为800，theta1约为-0.1，对应蓝色的斜线，当然它并不能很好地拟合这些数据。等高线图中，这个点与最小值（中心点）的距离很远，这是一个很高的代价（cost），因为拟合得并不好</p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304011238343.png" alt="image-20210304011238343"></p><p>另一个例子，这个点theta0大约为360，theta1为0，是蓝色线表示的一条平行线。当然它的代价也很大，比上个点要小一些。</p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210304011646560.png" alt="image-20210304011646560"></p><p><strong>Summary</strong></p><ul><li><p>我们真正想要的是一个高效的算法，来自动寻找代价函数 J 的最小值，对应的theta0 and theta1。</p></li><li><p>有更多的参数后，我们涉及更高维的图形，没有办法绘制图形，因为很难可视化。</p></li><li><p>我们需要利用软件去找到使函数最小的theta0和theta1。</p></li></ul><h3 id="2-5-Gradient-Descent"><a href="#2-5-Gradient-Descent" class="headerlink" title="2.5 Gradient Descent"></a>2.5 Gradient Descent</h3><p>思路：给定θ0和θ1的初始值，通常将设为0，在梯度下降算法中，不停地一点点改变θ0和θ1，来使 J(θ0,θ1)最小。</p><p>对θ0和θ1赋值，就相当于从这个函数表面上的某个点出发，把这个图像想象成一座山，你正站在一个点上，在梯度下降算法中，你旋转360°，看看周围，并问自己，如果我要在某个方向上走一小步，我想尽快下山，那么我应该朝什么方向迈步。</p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210305002044805.png" alt="image-20210305002044805" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210305002140272.png" alt="image-20210305002140272" style="zoom:50%;" /></p><p>但是如果起点不同，你可能会得到一个完全不同的局部最优解，这是梯度下降算法的一个特点。</p><p>梯度下降算法的定义：</p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210305002314334.png" alt="image-20210305002314334"></p><p>我们将会反复做这一步，直到收敛。</p><p>我们要更新参数θ_j，为θ_j 减去 α乘以这一部分</p><p>Notations:</p><ul><li>:= 表示赋值</li><li>a=b 表示判断ab是否相等</li><li>α被称为学习率(learning rate)，α用来控制梯度下降时，我们迈出多大的步子，如果α很大，梯度下降就很迅速</li><li>最后一部分是导数项</li></ul><p>梯度下降需要同时更新(Spontaneous update)θ0和θ1。右边的是错误的梯度下降算法。</p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210305004428576.png" alt="image-20210305004428576"></p><h3 id="2-6-Gradient-Descent-Intuition"><a href="#2-6-Gradient-Descent-Intuition" class="headerlink" title="2.6 Gradient Descent Intuition"></a>2.6 Gradient Descent Intuition</h3><p><strong>如何理解导数项？</strong></p><p>我们先假设只有一个参数 θ_1，并且θ_1是实数，那么  J(θ_1)的函数如下图所示：</p><ul><li>如果从上图的红点出发，导数项为那一个点的切线的斜率，而这个斜率是正数，α作为学习率也是正数，那么 θ_1赋值后会变小，即向左移动，更靠近最小值的点；</li><li>同理，如果从下图出发，导数项为负值，那么θ_1赋值后会变大，向右移动，也是更靠近最小值的点。</li></ul><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210305005542464.png" alt="image-20210305005542464"></p><p><strong>如何理解α？</strong></p><ul><li><p>如果α太小，需要很多步才能到达最低点（注意：越往后斜率越小）</p></li><li><p>如果α太大，那么梯度下降可能会越过最低点，甚至无法收敛或者发散</p></li></ul><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210305010049784.png" alt="image-20210305010049784"></p><p><strong>如果已经在局部最低点了呢？</strong></p><ul><li>那么斜率为0，导数项则为0，θ_1更新为本身，不会再改变</li></ul><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210305010222992.png" alt="image-20210305010222992"></p><p><strong>即使α不变，梯度下降法也可以收敛到局部最低点</strong></p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210308234402470.png" alt="image-20210308234402470"></p><p>随着越来越接近最低点，导数越来越小，梯度下降法移动的幅度会自动变得越来越小，知道最终移动幅度变的很小，你会发现，已经收敛到局部极小值了。</p><h3 id="2-7-Gradient-Descent-For-Linear-Regression"><a href="#2-7-Gradient-Descent-For-Linear-Regression" class="headerlink" title="2.7 Gradient Descent For Linear Regression"></a>2.7 Gradient Descent For Linear Regression</h3><p>梯度下降算法和线性回归模型</p><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210308234650379.png" alt="image-20210308234650379"></p><p><strong>如何将梯度下降算法代入到平方差代价函数中，使其最小化呢？</strong></p><ul><li>重要的是偏导数项是什么？</li><li>下图进行推导</li></ul><p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210309003956855.png" alt="image-20210309003956855"></p><p><strong>弹幕指路：</strong><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210309004751046.png" alt="image-20210309004751046"></p><ul><li>带回梯度下降算法，不断重复该过程直到收敛</li></ul><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210309004114921.png" alt="image-20210309004114921" style="zoom:80%;" /><ul><li>注意：当你用梯度下降时，确保θ_0和θ_1同时更新</li></ul><p>我们使用梯度下降法时，最容易遇到的问题是它容易陷入局部最优解</p><p>但是事实上，线性回归的代价函数总是这样一个碗状函数（凸函数 convex function），这个函数只有一个全局最优。当你计算这种代价函数的梯度下降时，它总是会收敛到全局最优。</p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210309004506784.png" alt="image-20210309004506784" style="zoom:50%;" /><p>刚才的算法，有时被称为<strong>Batch梯度下降算法</strong>，意味着每一步梯度下降，我们都遍历了训练集的所有样本，当计算偏导数时，我们计算总和。在每一个单独的梯度下降，我们最终计算m个训练样本的总和。</p><img src="https://raw.githubusercontent.com/xxcheng505/typora-image-repo/master/image-20210309005136634.png" alt="image-20210309005136634" style="zoom:80%;" /><p>还有一种解法，求解代价函数 J 最小值，而不需要使用梯度下降这样的迭代算法，被称为正规方程租方法。</p><p>事实上相比于正规方程组法，梯度下降适用于更大的数据集。</p><p> <strong>❀❀❀ 恭喜你学习了第一个机器学习算法！ ❀❀❀</strong></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning - Andrew Ng </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Learning Notes </tag>
            
            <tag> Andrew Ng </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>吴恩达机器学习笔记1</title>
      <link href="/2021/01/23/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/"/>
      <url>/2021/01/23/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/</url>
      
        <content type="html"><![CDATA[<h2 id="1-2-什么是机器学习"><a href="#1-2-什么是机器学习" class="headerlink" title="1.2 什么是机器学习"></a>1.2 什么是机器学习</h2><p><strong>Machine Learning Definition</strong></p><p>Arthur Samuel (1959). Machine Learning: Field of study that gives computers the ability to learn without being explicitly programmed.</p><p>Tom Mitchell (1998). Well-posed Learning Problem: A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.</p><p><strong>Machine Learning Algorithms”</strong></p><ul><li>Supervised Learning </li><li>Unsupervised Learning</li></ul><p>Others: Reinforcement learning, recommender systems.</p><p><strong>Also talk about:</strong> Practical advice for applying learning algorithms.</p><h2 id="1-3-Supervised-Learning"><a href="#1-3-Supervised-Learning" class="headerlink" title="1.3 Supervised Learning"></a>1.3 Supervised Learning</h2><p><u>Supervised Learning</u>： “right answers” given</p><p>​        ↓</p><p><u>Regression</u>: Predict continuous valued output</p><p><u>Classification</u>: Discrete valued output (0 or 1)</p><h2 id="1-4-Unsupervised-Learning"><a href="#1-4-Unsupervised-Learning" class="headerlink" title="1.4 Unsupervised Learning"></a>1.4 Unsupervised Learning</h2><p><strong>datasets:</strong> no labels or same labels</p><p>Cocktail party problem algorithm</p><p><code>[W,s,v] = svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x&#39;)</code></p><p>[source: Sam Roweis, Yair Weiss &amp; Eero Simoncelli]</p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning - Andrew Ng </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Learning Notes </tag>
            
            <tag> Andrew Ng </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>上帝爱世人</title>
      <link href="/2020/03/11/20200311-GodLovesAll/"/>
      <url>/2020/03/11/20200311-GodLovesAll/</url>
      
        <content type="html"><![CDATA[<p>2020年3月11日</p><p>上帝爱世人 ——</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Thoughts </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>相信未来</title>
      <link href="/2020/03/10/20200309%20Future/"/>
      <url>/2020/03/10/20200309%20Future/</url>
      
        <content type="html"><![CDATA[<p>2020年3月10日</p><p>相信未来——</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Thoughts </tag>
            
            <tag> Emotion </tag>
            
            <tag> LiZhi </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Don&#39;t forget</title>
      <link href="/2020/03/09/20200310%20Forget/"/>
      <url>/2020/03/09/20200310%20Forget/</url>
      
        <content type="html"><![CDATA[<p>2020年3月9日</p><p>我什么也没忘，但是有些事只适合收藏。</p><p>不能说，也不能想，却又不能忘……</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Thoughts </tag>
            
            <tag> Emotion </tag>
            
            <tag> Excerpt </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
